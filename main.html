<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Modelling and Data Analytics for Pharmaceutical Sciences</title>
    <meta charset="utf-8" />
    <meta name="author" content="St√©phane Guerrier" />
    <link href="main_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="main_files/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="main_files/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link href="main_files/tile-view-0.2.4/tile-view.css" rel="stylesheet" />
    <script src="main_files/tile-view-0.2.4/tile-view.js"></script>
    <link href="main_files/panelset-0.2.4/panelset.css" rel="stylesheet" />
    <script src="main_files/panelset-0.2.4/panelset.js"></script>
    <script src="main_files/clipboard-2.0.6/clipboard.min.js"></script>
    <link href="main_files/xaringanExtra-clipboard-0.2.4/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="main_files/xaringanExtra-clipboard-0.2.4/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <link href="main_files/xaringanExtra-extra-styles-0.2.4/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">










class: title-slide  
&lt;div class="my-logo-right"&gt;&lt;/div&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
 

# Introduction to Regression 

## Big Data Foundations

### .smallest[Dominique-L. Couturier, Cancer Research UK, University of Cambridge, üá¨üáß]
### .smallest[St√©phane Guerrier, Data Analytics Lab, University of Geneva, üá®üá≠]
### .smallest[Maria-Pia Victoria-Feser, Research Center for Statistics, University of Geneva, üá®üá≠]
### .smallest[Yuming Zhang, Data Analytics Lab, University of Geneva üá®üá≠]

&lt;br&gt;
&lt;br&gt;
&lt;img src="pics/liscence.png" width="30%" style="display: block; margin: auto;" /&gt;
.center[.tiny[License: [CC BY NC SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)]]

---

class: inverse, middle, center

# Introduction

---

# What is statistics?

.pull-left[
.smaller[.hi-purple[Statistics] is a science that uses mathematics and computer science to deal with the collection, analysis, interpretation, and presentation of masses of numerical data. Informally, it is the .pink[science of learning from data].]
&lt;img src="pics/stat.jpeg" width="90%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [luminousmen](luminousmen.com)]
]

.pull-right[
.smaller[.hi-purple[Statistics] is a crucial part of our life. However, .pink[statistical methods are often consciously (or not) misused]. This can lead to contradictory studies and conclusions (as seen during the current COVID-19 pandemic).]

&lt;img src="pics/data-torture.png" width="85%" style="display: block; margin: auto;" /&gt;

.tiny[Source: [Atoz Markets](https://atozmarkets.com/news/untold-reality-of-p-hacking-in-finance/)]
]

---

# How can statistics be useful?

.smaller[Statistics can be used (among others) to

1. .pink[Visualize data] (e.g. propagation of COVID-19 in different countries).
2. .pink[Understand and interpret data] (e.g. main causes of cancer). 
3. .pink[Assess the validity of a hypothesis] (e.g. is a drug working?).
4. .pink[Make predictions] (e.g. predicting unemployment or risk indices).]

.smaller[Learning more about statistics allows to 

1. Better understand arguments based on data.
2. Be able to apply critical thinking about statistics used as evidence.
3. Understand how statistical associations are used to evaluate claims (hypotheses) and assess causal connections.] 

.smaller[.purple[Understanding and knowing how to interpret statistical analyses is therefore becoming an increasingly vital skill.]]

---

# How to test a (scientific) hypothesis?

.center[
.purple["In god we trust, all others must bring data." &lt;sup&gt;.smallest[üëã]&lt;/sup&gt;]
]


- .smallest[To assess the .pink[validity of a (scientific) hypothesis], the scientific community (generally) agrees on a specific procedure.]
- .smallest[These hypotheses can be .pink[nearly anything], such as:]
  1. .smallest[Coffee consumption increases blood pressure. ]
  2. .smallest[Republican politicians are bad/good for the American Economy.]
  3. .smallest[A glass of red wine is as good as an hour at the gym.Ô∏è]
- .smallest[This procedure involves the design of an experiment and then the collection of data to compute a metric, called .hi.purple[p-value], which evaluates the adequacy between the data and your original hypothesis.]
- .smallest[There is generally .pink[a specific threshold] (typically 5%), and if the p-value falls below this threshold we can claim that we have statistically significant result(s) validating our hypothesis.]

.footnote[.smallest[üëã From W. Edwards Deming]]

---

# Statistics vs Truth ü§•

- .smallest[.pink[Statistically significant results are not necessarily the truth], as there isn't a threshold (e.g. 5%) that separates real results from the false ones.]
- .smallest[This procedure simply provides us with one piece of a puzzle that should be considered in the context of other evidence.]

&lt;img src="pics/medical_studies.png" width="50%" style="display: block; margin: auto;" /&gt;

.footnote[.smallest[üëã] Read the original article: "*This is why you shouldn't believe that exciting new medical study*" [here](https://www.vox.com/2015/3/23/8264355/research-study-hype).]

---

# How does it work?

.smallest[
- Statistical methods are based on several fundamental concepts, the most central of which is to consider the information available (in the form of data) resulting from a .pink[random process].
- As such, the data represent a .hi-purple[random sample] of a totally or conceptually accessible .hi-purple[population].
- Then, .pink[statistical inference] allows to infer the properties of a population based on the observed sample. This includes deriving estimates and testing hypotheses.
]

&lt;img src="pics/sampling.png" width="45%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [luminousmen](luminousmen.com)]

---

# Hypothesis testing

- In general (scientific) hypotheses can be translated into a set of (non-overlapping idealized) statistical hypotheses:

`$$H_0: \theta \color{#eb078e}{\in} \Theta_0 \  \text{ and } \ H_a: \theta \color{#eb078e}{\not\in} \Theta_0.$$`

- In a hypothesis test, the statement being tested is called the .hi-purple[null hypothesis] `\(\color{#373895}{H_0}\)`. A hypothesis test is designed to assess the strength of the evidence against the null hypothesis.
- The .hi-purple[alternative hypothesis] `\(\color{#373895}{H_a}\)` is the statement we hope or suspect to be true instead of `\(\color{#373895}{H_0}\)`.
- Each hypothesis excludes the other, so that one can exclude one in favor of the other using the data.
- To be "testable", hypothesis are built on quantities summarizing the question of interest.
- Very often, the summary quantities concern a .pink[mean], as for example the mean effect of a drug on patients.

---

# Hypothesis testing

- .pink[Example:] a drug represses the progression of cancer

`$$H_0: \mu_{\text{drug}} \color{#eb078e}{=} \mu_{\text{control}} \  \text{ and } \ H_a: \mu_{\text{drug}}  \color{#eb078e}{&lt;} \mu_{\text{control}}.$$`

&lt;img src="pics/great_pic.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Hypothesis testing

- To assess the validity of the alternative hypothesis `\(H_a\)`, one can only rely on random quantities that are called .pink[test statistics].
- Test statistics are random because for each different sample one could draw from the same population, the value of the test statistic is different.
- Statistical theory allows to calibrate the .pink[null probability distribution function] of the test statistic under the null hypothesis `\(H_0\)`.
- This null probability distribution function allows to determine if what is observed with the sample at hand, i.e. `\(T_{obs}\)`, is sufficiently probable or not under the null hypothesis.
- The metric that is used is the .hi.purple[p-value], which is compared to an a priori chosen .hi.purple[significance level] `\(\alpha\)`, usually `\(\alpha=5\%\)`.
- Hence, when .pink[rejecting] `\(H_0\)`, for a significance level of  `\(\alpha\)` means that there are `\(\alpha\%\)` chances that the alternative hypothesis `\(H_a\)` is NOT correct.

---

# What are p-values?

- .smallest[The .hi-purple[p-value] can be thought as the probability of observing a test statistic that is at least as extreme as actually observed, assuming that] `\(\small H_0\)` .smallest[is true.]
- .smallest[Informally, .pink[a p-value can be understood as a measure of plausibility of the null hypothesis given the data]. Small p-value indicates strong evidence against] `\(\small H_0\)`.
- .smallest[When the p-value is small enough (i.e. smaller than the significance level] `\(\small \alpha\)`.smallest[), one says that the test based on the null and alternative hypotheses is .pink[significant] or that the null hypothesis is rejected in favor of the alternative. This is generally what we want because it "verifies" our (research) hypothesis.]
- .smallest[When the p-value is not small enough, with the available data, we cannot reject the null hypothesis so nothing can be concluded.] ü§î
- .smallest[The obtained p-value summarizes somehow the .pink[incompatibility between the data and the model] constructed under the set of assumptions.]

.center[
.smaller.purple["Absence of evidence is not evidence of absence."] &lt;sup&gt;.smallest[üëã]&lt;/sup&gt;]


.footnote[.smallest[üëã] From the British Medical Journal.]

---

# How to understand p-values?

&lt;img src="pics/p_value.png" width="45%" style="display: block; margin: auto;" /&gt;

üëã .smallest[If you want to know more have a look [here](https://xkcd.com/1478/).]

---

# P-values may be controversial

.pink[P-values have been misused] many times because understanding what they mean is not intuitive!

&lt;div align="center"&gt;
&lt;iframe src="https://fivethirtyeight.abcnews.go.com/video/embed/56150342" width="675" height="380" scrolling="no" style="border:none;" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt; 

üëã .smallest[If you want to know more have a look [here](https://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/).]

---

class: inverse, middle, center

# Two-sample Location Tests

---

# .smaller[Two-sample location tests]

In practice, we often encounter problems where our goal is .pink[to compare the means (or locations) of two samples]. For example,
1. A scientist is interested in comparing the vaccine efficacy of the Pfizer-BioNTech and the Moderna vaccine.
2. A bank wants to know which of two proposed plans will most increase the use of its credit cards.
3. A psychologist wants to compare male and female college students' impression on a selected webpage.

We will discuss three .pink[two-sample location tests]:
1. .purple[Two independent sample Student's t-test]
2. .purple[Two independent sample Welch's t-test]
3. .purple[Two independent sample Mann-Whitney-Wilcoxon test]

---

# .smaller[Two independent sample Student's t-test]

Hypotheses for comparing the means of two groups .hi-purple2[A] and .hi-blue[B]:

`$$H_0: \color{#6A5ACD}{\mu_A} - \color{#06bcf1}{\mu_B} \color{#eb078e}{=} \mu_0 \ \ \ \ \text{and} \ \ \ \ H_a: \color{#6A5ACD}{\mu_A} - \color{#06bcf1}{\mu_B} \ \big[ \color{#eb078e}{&gt;} \text{ or } \color{#eb078e}{&lt;} \text{ or } \color{#eb078e}{\neq} \big] \ \mu_0.$$`
Usually one chooses `\(\mu_0=0\)` meaning that under `\(H_0\)` there is no difference between the two samples means.

Test statistic's distribution under `\(H_0\)`:

`$$\color{#b4b4b4}{T = \frac{(\overline{X}_{A}-\overline{X}_{B})-\mu_0}{s_{p}\sqrt{n_{A}^{-1}+n_{B}^{-1}}} \ {\underset{H_0}{\sim}} \ \text{Student}(n_{A}+n_{B}-2).}$$` 

- R function: 

.center[
`t.test(x = ..., y = ..., alternative = ..., var.equal = TRUE)`.]

---

# .smaller[Two independent sample Student's t-test]

One could also formulate the same test by considering a .pink[linear model] for each observation `\(\color{#6A5ACD}{X_{iA}},i=1,\ldots \color{#6A5ACD}{n_A}\)` and `\(\color{#06bcf1}{X_{iB}},i=1,\ldots,\color{#06bcf1}{n_B}\)`, or in short `\(X_{i(g)}\)`, where `\(g=\color{#6A5ACD}{A},\color{#06bcf1}{B}\)`, `\(i=1,...,n_{g}\)`.

Indeed, each observation in one of the groups can be considered as a realization of a random variable with mean `\(\mu_g\)`, and hence

`$$X_{i(g)} = \color{#eb078e}{\mu_{g}} + \varepsilon_{i(g)},$$`
where, for example, `\(\varepsilon_{i(g)} \overset{iid}{\sim} \mathcal{N}(0,\color{#eb078e}{\sigma^{2}})\)`.

Under `\(H_0\)` (equal means), then we can write

`$$X_{i(g)} = \color{#eb078e}{\mu_{g}} + \varepsilon_{i(g)} = \mu + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},\;\;\; \color{#6A5ACD}{\mu_A}\leq\mu\leq\color{#06bcf1}{\mu_B}$$` 

Then the null hypothesis becomes `\(H_0:\color{#6A5ACD}{n_A\delta_A}+\color{#06bcf1}{n_B\delta_B}=0\)`. 

---

# .smaller[Discussion - Student's t-test]


- This test relies on the following assumptions:
  - the observations in each group are spread around their respective means following a normal distribution `\(N(\mu_g,\sigma^2)\)`,
  - the spread around the mean is the same in each group, i.e. `\(\sigma^2\)`.
- These assumptions can hence be violated for example when
  - there are  .pink[outliers] in one or both groups, 
  - the sample distribution in each group is .pink[skewed],
  - the spread (variance) of the distribution is different in the two groups.
- In practice, the assumption of equal variance is hard to verify so .hi.pink[we recommend to avoid this test in practice].
- In that case, an alternative test is the .pink[Welch's t-test] (see later.)
- If outliers appear to be present, an alternative test is the .pink[Mann-Whitney-Wilcoxon test] (see later).

---

# .smaller[Two independent sample Welch's t-test]

The hypotheses are the same as for the Student t-test: 

`$$H_0: \color{#6A5ACD}{\mu_A} - \color{#06bcf1}{\mu_B} \color{#eb078e}{=} \mu_0 \ \ \ \ \text{and} \ \ \ \ H_a: \color{#6A5ACD}{\mu_A} - \color{#06bcf1}{\mu_B} \ \big[ \color{#eb078e}{&gt;} \text{ or } \color{#eb078e}{&lt;} \text{ or } \color{#eb078e}{\neq} \big] \ \mu_0,$$`
with generally `\(\mu_0=0\)`.

Test statistic's distribution under `\(H_0\)`:

`$$\color{#b4b4b4}{T = \frac{(\overline{X}_{A}-\overline{X}_{B})-\mu_0}{\sqrt{s^2_A/n_{A} + s^2_B/n_{B}}} \ {\underset{H_0}{\sim}} \ \text{Student}(df).}$$` 
which is different from the one used in the Student t-test (see later).

- R function: 

.center[
`t.test(x = ..., y = ..., alternative = ...)`.]

---

# .smaller[Two independent sample Welch's t-test]

The corresponding linear model remains 

`$$X_{i(g)} = \color{#eb078e}{\mu_{g}} + \varepsilon_{i(g)} = \mu + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},\;\;\; \color{#6A5ACD}{\mu_A}\leq\mu\leq\color{#06bcf1}{\mu_B},$$`
with associated null hypothesis `\(H_0:\color{#6A5ACD}{n_A\delta_A}+\color{#06bcf1}{n_B\delta_B}=0\)`.

The hypotheses, on the other hand, change to `\(\varepsilon_{i(g)} \overset{iid}{\sim} \mathcal{N}(0,\color{#eb078e}{\sigma^{2}_g})\)`, i.e. the variance is allowed to be different in each group.

- Notes:
  - The Welch's test strongly relies on the .pink[assumed absence of outliers].
  - The sample distribution should be at least .pink[approximately normal] with no strong skewness to ensure the reliability of the test.
  - Even if the equality of variance is true, there is not real loss in using the Welch's t-test compared to the Student's t-test.

---

# .smaller[Mann-Whitney-Wilcoxon test]

With the Mann-Whitney-Wilcoxon test, one assesses the difference between a more general location measure than the population mean for group .hi-purple2[A] and .hi-blue[B].

Let this location measure (or center) be denoted by respectively `\(\color{#6A5ACD}{\theta_{A}}\)` and `\(\color{#06bcf1}{\theta_{B}}\)`.

The hypotheses are 

`$$H_0: \color{#6A5ACD}{\theta_A} - \color{#06bcf1}{\theta_B} \color{#eb078e}{=} \theta_0 \ \ \ \ \text{and} \ \ \ \ H_a: \color{#6A5ACD}{\theta_A} - \color{#06bcf1}{\theta_B} \ \big[ \color{#eb078e}{&gt;} \text{ or } \color{#eb078e}{&lt;} \text{ or } \color{#eb078e}{\neq} \big] \ \theta_0,$$`
with in most cases `\(\theta_0=0\)`.

Test statistic is based on ranks:

`$$\color{#b4b4b4}{Z = \frac{\sum_{i=1}^{n_{B}}R_{i(g)}-[n_{B}(n_{A}+n_{B}+1)/2]}{\sqrt{n_{A}n_{B}(n_{A}+n_{B}+1)/12}},}$$`
.grey[where] `\(\color{#b4b4b4}{R_{i(g)}}\)` .grey[denotes the global rank of the] `\(\color{#b4b4b4}{i}\)`.grey[-th observation of group] `\(\color{#b4b4b4}{g}\)`.grey[.]


---

# .smaller[Mann-Whitney-Wilcoxon test]

- R function: 

.center[`wilcox.test(x = ..., y = ..., alternative = ...)`.]

- Notes:
  - The distribution of the test statistic under the null is elaborated and can be obtained by different methods (e.g. exact, asymptotic normal, ...). The details are beyond the scope of this class.
  - Since the test statistic is based on the global ranks of the data, i.e. one uses values form `\(1\)` to `\((\color{#6A5ACD}{n_A}+\color{#06bcf1}{n_B})\)` instead of the observed values, outliers can at most get the minimal or maximal rank, independently of the observed value.
  - Also, since the ranks are equally spaced, the possible skweness of the distribution within groups has no effect of the test statistic.
  - Hence, the Mann-Whitney-Wilcoxon test is said to be ".pink[robust]" to outliers and skweness.

---

# .smaller[Mann-Whitney-Wilcoxon test]

As for the previous tests, one can rewrite the the hypotheses relative to the following linear model

`$$X_{i(g)} = \color{#eb078e}{\theta_{g}} + \varepsilon_{i(g)} = \theta + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},\;\;\;\; \color{#6A5ACD}{\theta_A}\leq\theta\leq\color{#06bcf1}{\theta_B},$$`
where  `\(\varepsilon_{i(g)} \overset{iid}{\sim} (0,\color{#eb078e}{\sigma^{2}})\)`.

Then the null hypothesis becomes `\(H_0:\color{#6A5ACD}{n_A\delta_A}+\color{#06bcf1}{n_B\delta_B}=0\)`. 

Hence, the assumption does not require a normal distribution... However, if the distributions are symmetric, we have that `\(\color{#6A5ACD}{\theta_A = \mu_A}\)` and `\(\color{#06bcf1}{\theta_B = \mu_B}\)`.

Compared to the t-tests or Welch's t-test, the Mann-Whitney-Wilcoxon test is less powerful if their requirements (Gaussian and possibly same variances) are met. .hi.pink[Less powerful means that the probability of rejecting] `\({\color{#eb078e}{H_0}}\)` .hi.pink[when] `\({\color{#eb078e}{H_a}}\)` .hi.pink[is true, is smaller.]

---

# Example: Comparing diets A and B 

.panelset[
.panel[.panel-name[Procedure]
1. .purple[Define hypotheses:] `\(H_0: \mu_A = \mu_B\)` and `\(H_a: \mu_A \color{#e64173}{\neq} \mu_B\)`.
2. .purple[Define the significance level] `\(\color{#373895}{\alpha}\)`: we consider `\(\alpha = 5\%\)`.
3. Get the data and visualize them with e.g. boxplots, for checking which test is most suitable.
3. .purple[Compute the p-value] and compare it to `\(\alpha\)`. 
4. .purple[Conclusion:] reject or fail to reject the null hypothesis at the significance level of 5%.
]
.panel[.panel-name[Import]

```r
# Import data
diet = read.csv("data/diet.csv",row.names=1)
diet[sample(1:nrow(diet),9),]
```

```
#&gt;    gender age height diet.type initial.weight final.weight
#&gt; 74   Male  35    183         C             83         80.2
#&gt; 24   Male  40    190         A             88         84.5
#&gt; 65 Female  48    153         C             75         68.7
#&gt; 11 Female  60    173         A             72         70.5
#&gt; 57 Female  20    169         C             67         61.6
#&gt; 23   Male  39    166         A             87         81.9
#&gt; 46   Male  37    194         B             78         76.3
#&gt; 13 Female  41    163         A             72         68.4
#&gt; 43   Male  54    196         B             75         69.2
```
]
.panel[.panel-name[Select]

```r
# Compute weight loss
diet$weight.loss = diet$initial.weight - diet$final.weight

# Select diet
posA = diet$diet.type=="A"
posB = diet$diet.type=="B"

# Variable of interest
dietA = diet$weight.loss[posA]
dietB = diet$weight.loss[posB]
```
]
.panel[.panel-name[Graph]

```
#&gt; [1] "n(A)= 24 , n(B)= 25"
```

&lt;img src="pics/dietAB.png" width="60%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Welch]

```r
t.test(dietA, dietB)
```

```
#&gt; 
#&gt; 	Welch Two Sample t-test
#&gt; 
#&gt; data:  dietA and dietB
#&gt; t = 0.047594, df = 46.865, p-value = 0.9622
#&gt; alternative hypothesis: true difference in means is not equal to 0
#&gt; 95 percent confidence interval:
#&gt;  -1.320692  1.384692
#&gt; sample estimates:
#&gt; mean of x mean of y 
#&gt;     3.300     3.268
```
]
.panel[.panel-name[Mann-W-W]

```r
wilcox.test(dietA, dietB)
```

```
#&gt; 
#&gt; 	Wilcoxon rank sum test with continuity correction
#&gt; 
#&gt; data:  dietA and dietB
#&gt; W = 277, p-value = 0.6526
#&gt; alternative hypothesis: true location shift is not equal to 0
```
]
.panel[.panel-name[Results]
1. Welch's t-test appears suitable in this case (maybe also the Mann-Whitney-Wilcoxon test).
2. For the former, we obtain: p-value =  `\(96.22 \%\)` (see R output tab for details).
4. .purple[Conclusion:] We have p-value &gt; `\(\alpha\)` so we fail to reject the null hypothesis at the significance level of 5%.
]
.panel[.panel-name[(Student)]

```r
t.test(dietA, dietB, var.equal = TRUE)
```

```
#&gt; 
#&gt; 	Two Sample t-test
#&gt; 
#&gt; data:  dietA and dietB
#&gt; t = 0.0475, df = 47, p-value = 0.9623
#&gt; alternative hypothesis: true difference in means is not equal to 0
#&gt; 95 percent confidence interval:
#&gt;  -1.323275  1.387275
#&gt; sample estimates:
#&gt; mean of x mean of y 
#&gt;     3.300     3.268
```
]
]

---

class: inverse, middle, center

# Location Tests with Multiple Samples

---

# Problems with multiple samples

In practice, we often even encounter situations where we need to .pink[compare the means of more than 2 groups]. For example, we want to compare the weight loss efficacy of several diets, say diets .hi-purple2[A], .hi-blue[B], .hi-green[C]. The aim could, for example, be to evaluate the following hypothesis: `\(0 &lt; \color{#6A5ACD}{\mu_A} = \color{#06bcf1}{\mu_B} &lt; \color{#8bb174}{\mu_C}\)`. One intuitive (but .hi.pink[inappropriate]) approach could be:

1. Show that `\(\color{#8bb174}{\mu_C}\)` is greater than `\(\color{#6A5ACD}{\mu_A}\)` and `\(\color{#06bcf1}{\mu_B}\)` (i.e. Test 1:  `\(H_0\)`: `\(\color{#6A5ACD}{\mu_A} = \color{#8bb174}{\mu_C}\)`, `\(H_a\)`: `\(\color{#6A5ACD}{\mu_A} &lt; \color{#8bb174}{\mu_C}\)`; Test 2: `\(H_0\)`: `\(\color{#06bcf1}{\mu_B} = \color{#8bb174}{\mu_C}\)`, `\(H_a\)`: `\(\color{#06bcf1}{\mu_B} &lt; \color{#8bb174}{\mu_C}\)`). Here we hope to reject `\(H_0\)` in both cases.
2. Show that `\(\color{#6A5ACD}{\mu_A}\)` and `\(\color{#06bcf1}{\mu_B}\)` are greater than 0 (i.e. Test 3:  `\(H_0\)`: `\(\color{#6A5ACD}{\mu_A} = 0\)`, `\(H_a\)`: `\(\color{#6A5ACD}{\mu_A} &gt; 0\)`; Test 4: `\(H_0\)`: `\(\color{#06bcf1}{\mu_B} = 0\)`, `\(H_a\)`: `\(\color{#06bcf1}{\mu_B} &gt;0\)`). Here we also hope to reject `\(H_0\)` in both cases.
3. Compare `\(\color{#6A5ACD}{\mu_A}\)` and `\(\color{#06bcf1}{\mu_B}\)` (i.e. Test 5: `\(H_0\)`: `\(\color{#6A5ACD}{\mu_A} = \color{#06bcf1}{\mu_B}\)`, `\(H_a\)`: `\(\color{#6A5ACD}{\mu_A} \neq \color{#06bcf1}{\mu_B}\)`). Here we hope not to reject `\(H_0\)`. ‚ö†Ô∏è This does not imply that `\(\color{#6A5ACD}{\mu_A} = \color{#06bcf1}{\mu_B}\)` is true but at least the result would not contradict our theory.

---

# .smaller[Is there a problem in doing many tests?]

.purple[Are jelly beans causing acne? Maybe... but why only green ones?] ü§® 

&lt;img src="pics/green.png" width="45%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [xkcd](https://xkcd.com/882/)]
---

# .smaller[Are jelly beans causing acne?]

&lt;br&gt;
&lt;img src="pics/green1.png" width="85%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [xkcd](https://xkcd.com/882/)]

---

# .smaller[Maybe a specific color?]

&lt;br&gt;
&lt;img src="pics/green2.png" width="76%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [xkcd](https://xkcd.com/882/)]

---

# .smaller[Maybe a specific color?]

&lt;br&gt;
&lt;img src="pics/green3.png" width="75%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [xkcd](https://xkcd.com/882/)]

---

# .smaller[And finally...]

&lt;img src="pics/green.png" width="45%" style="display: block; margin: auto;" /&gt;
.tiny[Source: [xkcd](https://xkcd.com/882/)]

üëã .smallest[If you want to know more about this comics have a look [here](https://www.explainxkcd.com/wiki/index.php/882:_Significant).]

---

# .smaller[Multiple testing can be dangerous!]

- Remember that a p-value is .purple2[random] as its value depends on the data.
- If multiple hypotheses are tested, the chance of observing a rare event increases, and therefore, the chance to incorrectly reject a null hypothesis (i.e. making a Type I error) increases.
- For example, if we consider `\(k\)` (independent) tests (whose null hypotheses are all correct), we have

`$$\begin{align}
\alpha_k &amp;= \Pr(\text{reject } H_0 \text{ at least once}) \\
&amp;= 1 - \Pr(\text{not reject } H_0 \text{ test 1}) \times \ldots \times \Pr(\text{not reject } H_0 \text{ test k})\\
&amp;= 1 - (1-\alpha) \times \ldots \times (1-\alpha) = 1 - (1-\alpha)^k
\end{align}$$`

- Therefore, `\(\alpha_k\)` increases rapidly with `\(k\)` (e.g. `\(\alpha_1 = 0.05\)`, `\(\alpha_2 \approx 0.098\)`, `\(\alpha_{10} \approx 0.4013\)`, `\(\alpha_{100} \approx 0.9941\)`).
- Hence .pink[performing multiple tests, with the same or different data, is dangerous] ‚ö†Ô∏è (but very common! üòü) as it can leads to significant results, when actually there are none!

---

# .smaller[Possible solutions]

Suppose that we are interested in making `\(k\)` tests and that we want the probability of rejecting the null at least once (assuming the null hypotheses to be correct for all tests) `\(\alpha_k\)` to be equal to `\(\alpha\)` (typically 5%). Instead of using `\(\alpha\)` for the individual tests we will use `\(\alpha_c\)` (i.e. a corrected `\(\alpha\)`). Then, for `\(k\)` (potentially .purple2[dependent]) tests we have

`$$\begin{align}
\alpha_k &amp;= \alpha = \Pr(\text{reject } H_0 \text{ at least once}) \\
&amp;= \Pr(\text{reject } H_0 \text{ test 1} \; \cup \; \ldots \; \cup \; \text{reject } H_0 \text{ test k})\\
&amp;\color{#eb078e}{\leq} \sum_{i = 1}^k \Pr(\text{reject } H_0 \text{ test i}) = \alpha_c \times k.
\end{align}$$`

Solving for `\(\alpha_c\)` we obtain: `\(\color{#6A5ACD}{\alpha_c = \alpha/k}\)`, which is called .pink[Bonferroni correction]. By making use of the .pink[Boole's inequality], this approach does not require any assumptions about dependence among the tests or about how many of the null hypotheses are true.

---

# .smaller[Possible solutions]

The Bonferroni correction can be conservative if there are a large number of tests, as it comes at the cost of reducing the power of the individual tests (e.g. if `\(\alpha = 5\%\)` and `\(k = 20\)`, we get `\(\alpha_c = 0.05/20 = 0.25\%\)`). There exists a (slightly) "tighter" bound for `\(\alpha_k\)`, which is given by

`$$\alpha_k = \Pr(\text{reject } H_0 \text{ at least once}) \color{#eb078e}{\leq} 1 - (1 - \alpha_c)^k.$$`
Solving for `\(\alpha_c\)` we obtain: `\(\color{#6A5ACD}{\alpha_c = 1 - (1 - \alpha)^{1/k}}\)`, which is called .pink[Dunn‚Äì≈†id√°k correction]. This correction is (slightly) less stringent than the Bonferroni correction (since `\(1 - (1 - \alpha)^{1/k} &gt; \alpha/k\)` for `\(k \geq 2\)`).

There exist many other alternative methods for multiple testing corrections. It is important to mention that when `\(k\)` is large (say `\(&gt;\)` 100) the Bonferroni and Dunn‚Äì≈†id√°k corrections become inapplicable and methods based on the idea of .pink[False Discovery Rate] should be preferred. However, these recent methods are beyond the scope of this class.

---

# .smaller[Multiple-sample location tests]

To compare several means of different populations, a standard approach is to start our analysis by using the .pink[multiple-sample location tests]. More precisely, we proceed our analysis with the following steps:

  - .purple2[Step 1:] We first perform the multiple-sample location tests, where the null hypothesis states that all the locations are the same. If we cannot reject the null hypothesis, we stop our analysis here. Otherwise, we move on to Step 2.
  - .purple2[Step 2:] We compare the groups mutually (using `\(\alpha_c\)`) with two-sample location tests in order to verify our hypothesis.
  
We will discuss three .pink[multiple-sample location tests]:

 1. .purple2[Fisher's one-way ANalysis Of VAriance (ANOVA)]
 2. .purple2[Welch's one-way ANOVA]
 3. .purple2[Kruskal-Wallis test]

---

# Fisher's one-way ANOVA

.smallest[This test considers the following assumed model for G groups]
`$$\small X_{i(g)} = \color{#eb078e}{\mu_{g}} + \varepsilon_{i(g)} = \mu + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},$$`
.smallest[where] `\(\mu\)` .smallest[is the overall mean,] `\(\small g=1,\ldots, G\)`, `\(i=1,...,n_{g}\)`, `\(\small \varepsilon_{i(g)} \overset{iid}{\sim} \mathcal{N}(0,\color{#eb078e}{\sigma^{2}})\)` .smallest[so that] `\(\small H_0:\sum n_{g}\delta_{g}=0\)`. 

.smallest[This corresponds to:]
`$$\small H_0: \color{#6A5ACD}{\mu_1} \color{#eb078e}{=} \color{#06bcf1}{\mu_2} \color{#eb078e}{=} \ldots \color{#eb078e}{=} \color{#8bb174}{\mu_G} \ \ \ \ \text{and} \ \ \ \ H_a: \mu_i \color{#eb078e}{\neq} \mu_j \ \ \text{for at least one pair of} \ \ (i,j).$$`

.smallest[Test statistic's distribution under] `\(\small H_0\)`:

`\(\small \color{#b4b4b4}{F = \frac{N s^2_{\overline{X}}}{s_p^2} \ {\underset{H_0}{\sim}} \ \text{Fisher}(G-1, N-G),}\)`
.smallest[.grey[where]] `\(\small \color{#b4b4b4}{s^2_{\overline{X}} = \frac{1}{G-1} \sum_{g=1}^G \frac{n_g}{N} (\overline{X}_g - \overline{X})^2}\)`.smallest[.grey[,]] `\(\small \color{#b4b4b4}{s_p^2 = \frac{1}{N-G} \sum_{g=1}^G (n_g-1)s_g^2}\)`.smallest[.grey[,]] `\(\small \color{#b4b4b4}{N = \sum_{g=1}^G n_g}\)`.smallest[.grey[, and]] `\(\small \color{#b4b4b4}{\overline{X} = \frac{1}{N} \sum_{g=1}^G n_g \overline{X}_g}\)`.smallest[.grey[.]]

---

# Discussion - Fisher's one-way ANOVA

R function: 

.center[
`aov(response ~ groups, data = mydata)`.]

where `response` is the column in `mydata` of all the `\(X_{i(g)}\)` and `groups` is the column in `mydata`that identifies `\(g\)` (the groups).

- Notes:
  - The Fisher's test strongly relies on the .pink[assumed absence of outliers]. If outliers appear to be present the Kruskal-Wallis test (see later) is (probably) a better option.
  - For moderate and small sample sizes, the sample distribution should be at least .pink[approximately normal] with no strong skewness and .pink[equality of variance] within the groups.
  - In practice, the assumption of equal variance is hard to verify so .hi.pink[we recommend to avoid testing for this in practice].

---

# Welch's one-way ANOVA

.smallest[This test considers the same assumed model for G groups]
`$$\small X_{i(g)} = \color{#eb078e}{\mu_{g}} + \varepsilon_{i(g)} = \mu + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},$$`
.smallest[but with] `\(\small \varepsilon_{i(g)} \overset{iid}{\sim} \mathcal{N}(0,\color{#eb078e}{\sigma_g^{2}})\)` .smallest[and with the same ] `\(\small H_0:\sum n_{g}\delta_{g}=0\)`. 

.smallest[This corresponds to:]
`$$\small H_0: \color{#6A5ACD}{\mu_1} \color{#eb078e}{=} \color{#06bcf1}{\mu_2} \color{#eb078e}{=} \ldots \color{#eb078e}{=} \color{#8bb174}{\mu_G} \ \ \ \ \text{and} \ \ \ \ H_a: \mu_i \color{#eb078e}{\neq} \mu_j \ \ \text{for at least one pair of} \ \ (i,j).$$`

.smallest[Test statistic's distribution under] `\(\small H_0\)`:
`$$\small \color{#b4b4b4}{F^* = \frac{s^{*^2}_{\overline{X}}}{1+\frac{2(G-2)}{3\Delta}} \ {\underset{H_0}{\sim}} \ \text{Fisher}(G-1, \Delta),}$$`
.smallest[.grey[where]] `\(\small \color{#b4b4b4}{s^{*^2}_{\overline{X}} = \frac{1}{G-1} \sum_{g=1}^G w_g (\overline{X}_g - \overline{X}^*)^2}\)`.smallest[.grey[,]] `\(\small \color{#b4b4b4}{\Delta = [\frac{3}{G^2-1} \sum_{g=1}^G \frac{1}{n_g} (1-\frac{w_g}{\sum_{g=1}^G w_g})]^{-1}}\)`.smallest[.grey[,]] `\(\small \color{#b4b4b4}{w_g = \frac{n_g}{s_g^2}}\)`.smallest[.grey[, and]] `\(\small \color{#b4b4b4}{\overline{X}^* = \sum_{g=1}^G \frac{w_g\overline{X}_g}{\sum_{g=1}^G w_g}}\)`.smallest[.grey[.]]

---

# Discussion - Welch's one-way ANOVA

- R function: 

.center[
`oneway.test(response ~ groups, data = mydata)`.]

- Notes:
  - This test strongly relies on the .pink[assumed absence of outliers]. If outliers appear to be present the Kruskal-Wallis test (see later) is (probably) a better option.
  - For moderate and small sample sizes, the sample distribution should be at least .pink[approximately normal] with no strong skewness to ensure the reliability of the test.
  - This test does not require the variances of the groups to be equal. If the variances of all the groups are the same (which is rather unlikely in practice), the Welch's one-way ANOVA losses a little bit of power compared to the Fisher's one-way ANOVA.

---

# Kruskal-Wallis test

.smallest[This test considers the same assumed model for G groups]
`$$\small X_{i(g)} = \color{#eb078e}{\theta_{g}} + \varepsilon_{i(g)} = \theta + \color{#eb078e}{\delta_{g}} + \varepsilon_{i(g)},$$`
.smallest[but with]  `\(\small \varepsilon_{i(g)} \overset{iid}{\sim} \mathcal{N}(0,\color{#eb078e}{\sigma^{2}})\)` .smallest[and] `\(\small H_0: \sum n_{g}\delta_{g}=0\)`. 

.smallest[The] `\(\color{#eb078e}{\theta_{g}},g=1,\ldots,G\)` .smallest[are location measures, not necessarily means.] 

.smallest[This corresponds to:]
`$$\small H_0: \color{#6A5ACD}{\theta_1} \color{#eb078e}{=} \color{#06bcf1}{\theta_2} \color{#eb078e}{=} \ldots \color{#eb078e}{=} \color{#8bb174}{\theta_G} \ \ \ \ \text{and} \ \ \ \ H_a: \theta_i \color{#eb078e}{\neq} \theta_j \ \ \text{for at least one pair of} \ \ (i,j).$$`

.smallest[Test statistic's distribution under] `\(\small H_0\)`: `\(\small \color{#b4b4b4}{K = \frac{\frac{12}{N(N+1)} \sum_{g=1}^G \frac{\overline{R}_g}{n_g}-3(N-1)}{1-\frac{\sum_{v=1}^V{t_v^3-t_v}}{N^3-N}} \ {\underset{H_0}{\sim}} \mathcal{X}(G-1)}\)`.smallest[.grey[, where]] `\(\small \color{#b4b4b4}{\overline{R}_g = \frac{1}{n_g} \sum_{i=1}^{n_g} R_{i(g)}}\)` .smallest[.grey[with]] `\(\small \color{#b4b4b4}{R_{i(g)}}\)` .smallest[.grey[denoting the global rank of the]] `\(\small \color{#b4b4b4}{i^{th}}\)` .smallest[.grey[observation of group]] `\(\small \color{#b4b4b4}{g}\)`.smallest[.grey[,]] `\(\small \color{#b4b4b4}{V}\)` .smallest[.grey[is the number of different values/levels in]] `\(\small \color{#b4b4b4}{X}\)` .smallest[.grey[and]] `\(\small \color{#b4b4b4}{t_v}\)` .smallest[.grey[denotes the number of times a given value/level occurred in]] `\(\small \color{#b4b4b4}{X}\)`.smallest[.grey[.]]

---

# Discussion - Kruskal-Wallis test

- R function: 

.center[
`kruskal.test(response ~ groups, data = mydata)`.]

- Notes:
  - This test is ".pink[robust]" in the sense that it is not overly affected by skewness and outliers.
  - For the Kruskal-Wallis test to be comparable to the one-way ANOVAs (i.e. testing for the mean) we need to assume: (1) The distributions are symmetric, (2) the variances are the same. Then, we have `\(\color{#eb078e}{\theta_i = \mu_i, i=1,\ldots,G}\)`.
  - Compared to the one-way ANOVA, the Kruskal-Wallis test is less powerful if their requirements (Gaussian and possibly same variances) are met.

---

# .smaller[Example: Comparing diets A, B and C]

.panelset[
.panel[.panel-name[Procedure (1)]
To compare diets A, B, and C, we first test if there's any difference among these 3 diets.

1. .purple[Define hypotheses:] $$ H_0: \mu_A = \mu_B = \mu_C\ \ \ \ \text{and} \ \ \ \ H_a: H_0 \text{ is false, or}$$ $$ H_0: \theta_A = \theta_B = \theta_C \ \ \ \ \text{and} \ \ \ \ H_a: H_0 \text{ is false}.$$
2. .purple[Define] `\(\color{#6A5ACD}{\alpha}\)`: we consider `\(\alpha = 5\%\)`.
3. Get the data and visualize them with e.g. boxplots, for checking which test is most suitable.
3. .purple[Compute the p-value] and compare it to `\(\alpha\)`. 
4. .purple[Conclusion:] reject or fail to reject the null hypothesis at the significance level of 5%.
]
.panel[.panel-name[Import Data]

```r
# Import data and compute weight loss
diet = read.csv("data/diet.csv",row.names=1)
diet$weight.loss = diet$initial.weight - diet$final.weight

# Variable of interest
dietA = diet$weight.loss[diet$diet.type=="A"]
dietB = diet$weight.loss[diet$diet.type=="B"]
dietC = diet$weight.loss[diet$diet.type=="C"]

# Create data frame
mydata = data.frame(response = c(dietA, dietB, dietC), 
                 groups = c(rep("A", length(dietA)), 
                            rep("B", length(dietB)), 
                            rep("C", length(dietC))))
```
]
.panel[.panel-name[Graph]

```
#&gt; [1] "n(A)= 24 , n(B)= 25 , n(C)= 27"
```

&lt;img src="pics/dietABC.png" width="60%" style="display: block; margin: auto;" /&gt;
]
.panel[.panel-name[Welch's test]

```r
(anova_res = oneway.test(response ~ groups, data = mydata))
```

```
#&gt; 
#&gt; 	One-way analysis of means (not assuming equal variances)
#&gt; 
#&gt; data:  response and groups
#&gt; F = 5.2693, num df = 2.00, denom df = 48.48, p-value = 0.008497
```
]
.panel[.panel-name[Results]
1. Welch's test appears suitable in this case.
2. For the former, we obtain: p-value =  0.85%.
4. .purple[Conclusion:] We have p-value &lt; `\(\alpha\)` so we reject the null hypothesis at the significance level of 5%.
]
]
---

# .smaller[Example: Comparing diets A, B and C]

.panelset[
.panel[.panel-name[Procedure (2)]
1. .purple[Define hypotheses:] `$$H_0: \mu_A = \mu_B \ \ \ \ \text{and} \ \ \ \ H_a: \mu_A &lt; \mu_B. \\ H_0: \mu_A = \mu_C \ \ \ \ \text{and} \ \ \ \ H_a: \mu_A &lt; \mu_C. \\ H_0: \mu_B = \mu_C \ \ \ \ \text{and} \ \ \ \ H_a: \mu_B &lt; \mu_C.$$`
2. .purple[Define significance level]: we consider `\(\alpha/3 = 5\%/3 \approx 1.67\%\)` for each pair of hypotheses.
3. .purple[Compute the p-values] and compare them to `\(\alpha/3\)`. 
4. .purple[Conclusion:] reject or fail to reject the null hypotheses at the .pink["familywise"] significance level of 5%.
]
.panel[.panel-name[Analysis AB]

```r
test.AB=t.test(dietA, dietB, alternative = "less")
test.AB
```

```
#&gt; 
#&gt; 	Welch Two Sample t-test
#&gt; 
#&gt; data:  dietA and dietB
#&gt; t = 0.047594, df = 46.865, p-value = 0.5189
#&gt; alternative hypothesis: true difference in means is less than 0
#&gt; 95 percent confidence interval:
#&gt;      -Inf 1.160216
#&gt; sample estimates:
#&gt; mean of x mean of y 
#&gt;     3.300     3.268
```
]
.panel[.panel-name[Analysis AC]

```r
test.AC=t.test(dietA, dietC, alternative = "less")
test.AC
```

```
#&gt; 
#&gt; 	Welch Two Sample t-test
#&gt; 
#&gt; data:  dietA and dietC
#&gt; t = -2.8462, df = 48.862, p-value = 0.003225
#&gt; alternative hypothesis: true difference in means is less than 0
#&gt; 95 percent confidence interval:
#&gt;      -Inf -0.75944
#&gt; sample estimates:
#&gt; mean of x mean of y 
#&gt;  3.300000  5.148148
```
]
.panel[.panel-name[Analysis BC]

```r
test.BC=t.test(dietB, dietC, alternative = "less")
test.BC
```

```
#&gt; 
#&gt; 	Welch Two Sample t-test
#&gt; 
#&gt; data:  dietB and dietC
#&gt; t = -2.7858, df = 49.436, p-value = 0.003776
#&gt; alternative hypothesis: true difference in means is less than 0
#&gt; 95 percent confidence interval:
#&gt;        -Inf -0.7488193
#&gt; sample estimates:
#&gt; mean of x mean of y 
#&gt;  3.268000  5.148148
```
]
.panel[.panel-name[Conclusion]
We have: `\(p_{AB} \approx 51.89\%\)` for the Welch t-test between diets A and B; `\(p_{AC} \approx 0.32\%\)` for the Welch t-test between diets A and C; `\(p_{BC} \approx 0.38\%\)` for the Welch t-test between diets B and C.

.purple[Conclusion:] Since `\(p_{AB} &gt; 1.67\%\)`, `\(p_{AC} &lt; 1.67\%\)` and `\(p_{BC} &lt; 1.67\%\)`, we fail to reject `\(H_0: \mu_A = \mu_B\)` and reject both `\(H_0: \mu_A = \mu_C\)` and `\(H_0: \mu_B = \mu_C\)` at the .pink["familywise"] significance level of `\(\small 5\%\)`.
]

] 

---

class: inverse, middle, center

# Linear Regression

---

# .smallest[Motivating Example: Reading Ability]

.panelset[
.panel[.panel-name[Problem]
An educator believes that .purple2[new directed reading activities] in the classroom can help elementary school students (6-12 years old) improve their reading ability. She arranged a pilot study where some students (chosen at random) of age 6 start to take part in these activities .hi-purple2[(treatment group)], meanwhile other students continue with the .pink[classical curriculum] .hi-pink[(control group)]. The educator wishes to evaluate the effectiveness of these activities so all students are given a Degree of Reading Power (DRP) test, which assesses their reading ability. 

.pink[Can we conclude that these new directed reading activities can help elementary school students improve their reading ability?]
]
.panel[.panel-name[Data]


```r
dat = read.csv("data/reading.csv")
control = dat$score[dat$group == "Control"]
head(control)
```

```
#&gt; [1] 59.98881 60.39487 70.02395 41.47707 29.15497 58.84009
```

```r
treatment = dat$score[dat$group == "Treatment"]
head(treatment)
```

```
#&gt; [1] 36.80558 36.70201 59.89362 31.72289 41.04241 51.82160
```

]
.panel[.panel-name[Graph]

&lt;img src="pics/boxplot_reading.png" width="80%" style="display: block; margin: auto;" /&gt;

]
.panel[.panel-name[Test]
1. .purple[Define hypotheses:] `\(H_0: \mu_T = \mu_C\)` and `\(H_a: \mu_T &gt; \mu_C\)`.
2. .purple[Define] `\(\color{#6A5ACD}{\alpha}\)`: We consider `\(\alpha = 5\%\)`.
3. .purple[Compute p-value]: p-value = `\(97.12\%\)` (see R code tab for details).
4. .purple[Conclusion:] We have p-value &gt; `\(\alpha\)` so we cannot reject the null hypothesis at the significance level of 5%. 

.hi-pink[Remark:] Notice that the p-value for the test with opposite hypotheses is actually `\(100\%-97.12\%=2.88\% &lt; \alpha\)` (Why? ü§î). So we conclude, at the significance level of 5%, that the classical curriculum without these new directed reading activities actually improve students' reading ability more compared to the curriculum with these activities. üò¨
]
.panel[.panel-name[`R` Code ]

```r
t.test(treatment, control, alternative = "greater")
```

```
#&gt; 
#&gt; 	Welch Two Sample t-test
#&gt; 
#&gt; data:  treatment and control
#&gt; t = -1.9497, df = 43.527, p-value = 0.9712
#&gt; alternative hypothesis: true difference in means is greater than 0
#&gt; 95 percent confidence interval:
#&gt;  -10.77571       Inf
#&gt; sample estimates:
#&gt; mean of x mean of y 
#&gt;  44.77579  50.56302
```
]
]

---

# .smaller[Is our analysis comprehensive?]

The educator points out that only students of 6-8 years old have participated in the new directed reading activities. In other words, in the sample she collected, the students in the treatment group are only of age 6 to 8, whereas the students in the control group vary from 6 to 12 years old. .pink[Is age a potential explanation to the difference we observe among the students' reading ability?]

To make sure that the analysis is reliable, she includes the age information of the students, which can be accessed as follows:


```r
treatment_age = dat$age[dat$group == "Treatment"]
control_age   = dat$age[dat$group == "Control"]
```

---

# .smaller[Should age be taken into account?]

&lt;img src="pics/reading_points.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Regression analysis]

- Regression analysis corresponds to a set of statistical methods for estimating the .pink[relationships] between a response variable `\(Y\)` of primary interest (also called the *outcome variable*) and some explanatory variables `\(X_1, \ldots, X_p\)` (also called *covariates*, *regressors*, *features* or *predictors*).
- The relationship between the response variable `\(Y\)` and the covariates is not .purple2[deterministic] and we model the .purple2[conditional expected value] (i.e. `\(\mathbb{E}[Y | X_1, \ldots, X_p]\)`).
- Therefore, we consider the following (general) model:
`$$Y_i = \mathbb{E}[Y_i | X_{i1}, \ldots, X_{ip}] + \varepsilon_i,$$`where `\(\mathbb{E}[\varepsilon_i] = 0\)` and `\(i=1,\ldots,n\)`.
- .pink[Example]: `\(\mathbb{E}[\text{reading abilities}_i| \ \text{age}_i, \text{treatment}_i, \ldots]\)`.

---

# .smaller[Linear regression]

- The most common form of regression analysis is .pink[linear regression], in which 
the conditional expected value `\(\color{#373895}{\mathbb{E}[Y | X_1, \ldots, X_p]}\)` takes the form
`$$\color{#373895}{\mathbb{E}[Y_i| X_{i1}, \ldots, X_{ip}]} = \color{#373895}{\beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p}.$$`
and `\(\color{#eb078e}{\varepsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^2)}\)`.
- Our general model can be expressed as
`$$Y_i = \color{#373895}{\mathbb{E}[Y_i | X_{i1}, \ldots, X_{ip}]} + \varepsilon_i = \color{#373895}{\beta_0 + \sum_{j = 1}^p \beta_j X_{ij}} + \varepsilon_i,$$`
and therefore,
`$$Y_i \color{#eb078e}{ \overset{iid}{\sim} \mathcal{N}}\left(\color{#373895}{\beta_0 + \sum_{j = 1}^p \beta_j X_{ij}}, \color{#eb078e}{\sigma^2}\right).$$`

---

# .smaller[Linear regression]

Therefore, this approach makes two (.pink[strong]) assumptions:

1. The conditional expected value `\(\mathbb{E}[Y | X_1, \ldots, X_p]\)` is assumed to be a linear function of the covariates.
2. The errors are assumed to be `\(iid\)` Gaussian, i.e. `\(\color{#eb078e}{\varepsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^2)}\)`, at least when the sample size is small to medium.

‚ö†Ô∏è .pink[In practice, it is important to assess if these assumptions are plausible.]

The parameters of the model (i.e. `\(\beta_0, \beta_1, \ldots, \beta_p\)` and `\(\sigma^2\)`) can be estimated by several methods. The most commonly used is the .pink[least squares] approach where `\(\beta_0, \beta_1, \ldots, \beta_p\)` are chosen such that

`$$\small \sum_{i = 1}^n \varepsilon_i^2 = \sum_{i = 1}^n \left(Y_i- \mathbb{E}[Y_i | X_{i1}, \ldots, X_{ip}]\right)^2 = \sum_{i = 1}^n \left(Y_i- \beta_0 - \sum_{j = 1}^p \beta_j X_{ij}\right)^2$$`
is .pink[minimized], which then allows to estimate `\(\sigma^2\)` further on.

---

# .smaller[Anscombe's quartet]

&lt;img src="pics/Anscombe.png" width="80%" style="display: block; margin: auto;" /&gt;

üëã .smallest[Source: [Wikipedia](https://en.wikipedia.org/wiki/Anscombe%27s_quartet).]

---

# .smaller[Example: Reading ability assessment]

In the reading ability example, we can formulate a linear regression model (without interaction) as follows:
`$$\color{#e64173}{\text{Score}_i} = \beta_0 + \beta_1 \color{#6A5ACD}{\text{Group}_i} + \beta_2 \color{#20B2AA}{\text{Age}_i} + \epsilon_i, \quad \epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^2).$$`
- `\(\color{#e64173}{\text{Score}_i}\)`: score of the DRP test of the `\(i\)`-th student.
- `\(\color{#6A5ACD}{\text{Group}_i}\)`: indicator of participation of the new directed reading activities for the `\(i\)`-th student (i.e. `\(\color{#6A5ACD}{\text{Group}_i} = 1\)` if participate and `\(\color{#6A5ACD}{\text{Group}_i} = 0\)` if not participate).
- `\(\color{#20B2AA}{\text{Age}_i}\)`: age of the `\(i\)`-th student (related to *.turquoise[time since start of treatment]*). 

With this model the two groups can be compared as the age effect is taken into account. The goal of the educator is now to assess if `\(\beta_1\)` is .pink[significantly larger than 0].

---

# .smaller[Example: Reading ability assessment]

.panelset[
.panel[.panel-name[R Code]

.pink[R function]:  `lm(y ~ x1 + ... + xp, data = mydata)`.

Here is the code for our example:


```r
# Import data (if you haven't already)
dat = read.csv("data/reading.csv")

# Fit linear regression model
mod1 = lm(score ~ group + age, data = dat)
summary(mod1)
```

]
.panel[.panel-name[Output]

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = score ~ group + age, data = dat)
#&gt; 
#&gt; Residuals:
#&gt;    Min     1Q Median     3Q    Max 
#&gt; -9.247 -3.532  0.423  3.180 12.569 
#&gt; 
#&gt; Coefficients:
#&gt;                Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)     -7.8639     3.3238  -2.366   0.0214 *  
#&gt; groupTreatment   6.3771     1.3931   4.578  2.6e-05 ***
#&gt; age              6.6457     0.3695  17.985  &lt; 2e-16 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 4.447 on 57 degrees of freedom
#&gt; Multiple R-squared:  0.8586,	Adjusted R-squared:  0.8536 
#&gt; F-statistic:   173 on 2 and 57 DF,  p-value: &lt; 2.2e-16
```
]
]

---

# .smaller[Interpretation of coefficients]

We can obtain the estimated coefficients. Specifically,
- `\(\hat{\beta}_0 = -7.8639\)` represents the estimated baseline average score of the DRP test at birth (but what does it mean? ü§®).
- `\(\hat{\beta}_1 = 6.3771\)` means that .pink[for a student of the same age], participating in the new directed reading activities is estimated to increase their average score of the DRP test by 6.3771.
- `\(\hat{\beta}_2 = 6.6457\)` means that .pink[when a student receives the same treatment] (either participate or not in the activities), their average score increases by 6.6457 as they become 1 year older. 

Regression coefficients represent the mean change in the response variable .purple[for one unit of change] in the predictor variable .purple[while holding other covariates in the model constant.]

---

# .smaller[Interpretation of coefficient p-values]

- We notice that for each coefficient `\(\beta_j\)`, there is a corresponding p-value associated to the (Wald t-)test of `\(H_0: \beta_j = 0\)` and `\(H_a: \beta_j \neq 0\)`. 
- .pink[A covariate with a small p-value (typically smaller than 5%) is considered to be a significant (meaningful) addition to the model], as changes in the values of such covariate can lead to changes in the response variable. 
- On the other hand, a large p-value (typically larger than 5%) suggests that the corresponding covariate is not (significantly) associated with changes in the response or that we don't have enough evidence (data) to show its effect.
- In this example, the coefficient p-value associated to the `group` covariate is `\(2.6 \times 10^{-3}\)`%. .purple[This suggests that taking into account the effect of `age`, the reading abilities of the students receiving the treatment is significantly .hi.purple[different] from the control group, at the significance level of 5%.] But this is not what we want! 

---

# .smaller[Interpretation of coefficient p-values]

In the linear regression output, the coefficient p-value (which we denote as `\(p\)` below) corresponds to a two-sided test. We can use this result to compute the p-value of a one-sided test using the following relations:

|               | `\(\small H_a: \beta_j&gt;0\)`   | `\(\small H_a: \beta_j&lt;0\)`  |
| ------------- |:-------------:| :-----:|
| `\(\small \hat{\beta_j}&gt;0\)`     | `\(\small p/2\)` | `\(\small 1-p/2\)` |
| `\(\small \hat{\beta_j}&lt;0\)`      | `\(\small 1-p/2\)`      |   `\(\small p/2\)` |

In our example, `\(\beta_1 = 6.3771\)` and `\(p=2.6 \times 10^{-3}\%\)`. So the p-value of the test with hypotheses `\(H_0: \beta_1=0\)` and `\(H_a: \beta_1&gt;0\)` is `\(2.6 \times 10^{-3}\% /2 \approx 1.3 \times 10^{-3}\%  &lt; \alpha\)`. So we can conclude that these new directed reading activities can .pink[significantly improve] students' reading ability compared to classical curriculum. 

.purple2[However, is our model plausible?] ü§î

---

# .smaller[How good is our model? ü§î]

&lt;img src="pics/points_with_mod1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Could we use the] `\(R^2\)`.smaller[?]

- The .pink[coefficient of determination], denoted as `\(R^2\)` and often referred to as R-squared, corresponds to the proportion of the variance in the response variable that is "explained" by the model.  
- `\(\color{#6A5ACD}{R^2}\)` .purple[gives certain information about the goodness of fit of a model.] It measures how well the regression predictions approximate the real data points. An `\(R^2\)` of 1 indicates that the regression predictions perfectly fit the data.
-  However, the value of `\(R^2\)` is .pink[not related to the adequacy of our model to the data].
- ‚ö†Ô∏è Moreover, adding new covariates to the current model .purple[always] increases `\(R^2\)`, whether the additional covariates are significant or not. Therefore, `\(R^2\)` alone cannot be used as a meaningful comparison of models with different covariates.
- The .pink[adjusted] `\(\color{#e64173}{R^2}\)` is a modification of `\(R^2\)` that aims to limit this issue. 

---

# .smaller[Rexthor, the Dog-Bearer!]

&lt;img src="pics/linear_regression.png" width="90%" style="display: block; margin: auto;" /&gt;

üëã .smallest[If you want to know more have a look [here](https://www.explainxkcd.com/wiki/index.php/1725:_Linear_Regression).]

---

# .smaller[Model diagnostic]

&lt;img src="pics/diag_mod1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Model diagnostic] ‚ö†Ô∏è

&lt;img src="pics/diag_mod1_v2.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Model diagnostic] ‚ö†Ô∏è

&lt;img src="pics/diag2_mod1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Model diagnostic]

&lt;img src="pics/qqplot_mod1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Let's update our model]

Our results suggest that the students of the group participating in these new directed reading activities progress more rapidly (which is actually more reasonable than our initial model ü§î). Therefore, we modify our model by adding an interaction term:

`$$\color{#e64173}{\text{Score}_i} = \beta_0 + \beta_1 \color{#6A5ACD}{\text{Group}_i} + \beta_2 \color{#20B2AA}{\text{Age}_i} + \beta_3 \color{#6A5ACD}{\text{Group}_i}\color{#20B2AA}{\text{Age}_i} + \epsilon_i, \quad \epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^2).$$`
- `\(\color{#e64173}{\text{Score}_i}\)`: score of the DRP test of the `\(i\)`-th student.
- `\(\color{#6A5ACD}{\text{Group}_i}\)`: indicator of participation of the new directed reading activities for the `\(i\)`-th student (i.e. `\(\color{#6A5ACD}{\text{Group}_i} = 1\)` if participate and `\(\color{#6A5ACD}{\text{Group}_i} = 0\)` if not participate).
- `\(\color{#20B2AA}{\text{Age}_i}\)`: age of the `\(i\)`-th student (related to *.turquoise[time since start of treatment]*),

The goal of the educator is now to assess if `\(\beta_1\)` and/or `\(\beta_3\)` are .pink[significantly larger than 0].

---

# .smaller[Example: Reading ability assessment]

.panelset[
.panel[.panel-name[R Code]

Here is the code to fit our second model:


```r
# Import data (if you haven't already)
dat = read.csv("data/reading.csv")

# Fit linear regression model
mod2 = lm(score ~ group*age, data = dat)
summary(mod2)
```

]
.panel[.panel-name[Output]

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = score ~ group * age, data = dat)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -8.5033 -2.3708  0.4076  2.5478 10.1024 
#&gt; 
#&gt; Coefficients:
#&gt;                    Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)         -3.2489     2.7903  -1.164    0.249    
#&gt; groupTreatment     -36.0307     7.5392  -4.779 1.31e-05 ***
#&gt; age                  6.1207     0.3108  19.693  &lt; 2e-16 ***
#&gt; groupTreatment:age   5.9539     1.0468   5.688 4.86e-07 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 3.572 on 56 degrees of freedom
#&gt; Multiple R-squared:  0.9104,	Adjusted R-squared:  0.9056 
#&gt; F-statistic: 189.6 on 3 and 56 DF,  p-value: &lt; 2.2e-16
```
]
]

---

# .smaller[Interpretation of coefficients]

We can obtain the estimated coefficients. Specifically,
- `\(\hat{\beta}_0 = -3.2489\)` represents the estimated baseline average score of the DRP test at birth (but *again* what does it mean? üôÑ)
- `\(\hat{\beta}_1 = -36.0307\)` means that .pink[for a student of the same age], participating in the new directed reading activities is estimated to decrease their average score of the DRP test by 36.0307 (does this make sense? üßê).
- `\(\hat{\beta}_2 = 6.1207\)` means that for students not participating to the new directed reading activities, their average score increases by 6.1207 as they become 1 year older.
- `\(\hat{\beta}_3 = 5.9539\)` means that the average score of students participating in the new directed reading activities increases by 5.9539 as they become 1 year older .pink[compared to the other students]. This means that the average score of students participating to the new program increases by 12.0746 (i.e. 6.1207 + 5.9539) as they become 1 year older.

---

# .smaller[Model fit]

&lt;img src="pics/points_with_mod2.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Model diagnostic]

&lt;img src="pics/diag_mod2_v2.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Model diagnostic]

&lt;img src="pics/diag2_mod2.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Model diagnostic]

&lt;img src="pics/qqplot_mod2.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Model selection]

In general, we prefer a .pink[parsimonious approach to modeling] in the sense that we only choose a more complex model if the benefits are ".pink[substantial]"&lt;sup&gt;.smallest[üëã]&lt;/sup&gt;. We want our model to be such that:

1.  The model fits the data well.
2.  Avoid (excessive) overfitting.

Naturally these two objectives are .pink[contradictory] and there are many ways to select a suitable model (actually this is one of the most active areas of research in modern Statistics). In this class, we will only consider one (simple) approach based on the .pink[Akaike Information Criterion (AIC)]. This criterion corresponds to an .purple2[estimator of prediction error] and thereby .purple2[relative quality of statistical models for a given set of data].

.footnote[.smallest[üëã  This point of view is based on .pink[Occam's razor] (or law of parsimony), the problem-solving principle stipulating that ".purple2[the simplest explanation is usually the right one]".]]

---

# .smaller[Model selection]

In `R`, the AIC can be computed for a given model (i.e. use the output of the function `lm(...)` in `AIC(model)`.) For example, we can compare the AIC of the previously considered models as follows:


```r
AIC(mod1)     # First model (no interaction)
```

```
#&gt; [1] 354.2688
```

```r
AIC(mod2)     # Second model (with interaction)
```

```
#&gt; [1] 328.9095
```

As expected, these results suggest that the second model is more appropriate. .pink[But should we further improve it?]

---

# .smaller[Let's update our model (again)]

It should be reasonable that the average reading scores of the two groups are the same at the start of the program.

`$$\color{#e64173}{\text{Score}_i} = \beta_0 + \beta_1 \color{#20B2AA}{(\text{Age}_i - 6)}  + \beta_2 \color{#6A5ACD}{\text{Group}_i}\color{#20B2AA}{(\text{Age}_i - 6)} + \epsilon_i, \quad \epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^2).$$`
- `\(\color{#e64173}{\text{Score}_i}\)`: score of the DRP test of the `\(i\)`-th student.
- `\(\color{#6A5ACD}{\text{Group}_i}\)`: indicator of participation of the new directed reading activities for the `\(i\)`-th student (i.e. `\(\color{#6A5ACD}{\text{Group}_i} = 1\)` if participate and `\(\color{#6A5ACD}{\text{Group}_i} = 0\)` if not participate).
- `\(\color{#20B2AA}{\text{Age}_i - 6}\)`: corresponds to the time since start of treatment of the `\(i\)`-th student. 

With this model the two groups can be compared as the age effect is taken into account. The goal of the educator now is (.pink[only!]) to assess if `\(\beta_1\)` is .pink[significantly larger than 0].

---

# .smaller[Example: Reading ability assessment]

.panelset[
.panel[.panel-name[R Code]

Here is the code to fit our third model:


```r
# Import data (if you haven't already)
dat = read.csv("data/reading.csv")
dat$age_minus_6 = dat$age - 6

# Fit linear regression model
mod3 = lm(score ~ age_minus_6 + group:age_minus_6, data = dat)
summary(mod3)
```

]
.panel[.panel-name[Output]

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = score ~ age_minus_6 + group:age_minus_6, data = dat)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -8.5095 -2.4182  0.4826  2.6312 10.0853 
#&gt; 
#&gt; Coefficients:
#&gt;                            Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)                 33.3504     0.7904  42.192  &lt; 2e-16 ***
#&gt; age_minus_6                  6.1522     0.2604  23.625  &lt; 2e-16 ***
#&gt; age_minus_6:groupTreatment   5.8103     0.7157   8.119 4.37e-11 ***
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 3.542 on 57 degrees of freedom
#&gt; Multiple R-squared:  0.9103,	Adjusted R-squared:  0.9072 
#&gt; F-statistic: 289.2 on 2 and 57 DF,  p-value: &lt; 2.2e-16
```
]
.panel[.panel-name[AIC]

```r
AIC(mod1)
```

```
#&gt; [1] 354.2688
```

```r
AIC(mod2)
```

```
#&gt; [1] 328.9095
```

```r
AIC(mod3)
```

```
#&gt; [1] 326.9479
```
]
]

---

# .smaller[Model fit]

&lt;img src="pics/points_with_mod3.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Model diagnostic]

&lt;img src="pics/diag_mod3_v2.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Model diagnostic]

&lt;img src="pics/diag2_mod3.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Model diagnostic]

&lt;img src="pics/qqplot_mod3.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# .smaller[Concluding remarks]

- The last model we consider appears to fit the data, avoids overfitting and allows to answer whether the new reading activities are of interest. Indeed, the programs significantly improve the reading performance of the students (e.g. 5.81 more per year compared to control, p-value &lt; 5%).
- Our model .pink[assumes a linear relationship] between the response and the covariates. However, this may be incorrect.
- Our model .pink[only considers independent data] (which may not be correct here).
- Finally, linear regression .pink[should not be used to extrapolate], i.e. to estimate beyond the original observation range. For example, if we consider a 100 year-old person in this reading ability example, we would predict (using the third model) that the corresponding score of the DRP test would be 1157.59 and 611.45, respectively, with and without these activities. Does it really make sense? üòØ

---

# .smaller[Extrapolating]

&lt;img src="pics/extrapolating.png" width="90%" style="display: block; margin: auto;" /&gt;

üëã .smallest[If you want to know more have a look [here](https://www.explainxkcd.com/wiki/index.php/605:_Extrapolating).]

---

# .smaller[Exercise: Crime Rate]

.panelset[
.panel[.panel-name[Problem]
A police governor is interested in studying the effects of various variables by the US states on the .hi-pink[crime rate]. To study this issue, 47 crime rate data were collected, together with the following variables: `Youth` (number of males aged 18-24 per 1k), `Southern` (southern state, 1=yes, 0=no), `Education` (education time), `Expenditure` (expenditure on police), `LabourForce` (number of young men employed per 1k), `Males` (number of males per 1k females), `StateSize` (state size in hundred thousands), `HighYouthUnemploy` (1=yes, 0=no), `Wage` (median weekly wage), and `BelowWage` (number of families below half wage per 1k). .pink[Based on this data, can you find a suitable model to predict the crime rate?]
]
.panel[.panel-name[Import]

```r
# Import data
crime = read.csv("data/crime.csv", row.names=1)
head(crime)
```

```
#&gt;   CrimeRate Youth Southern Education Expenditure LabourForce Males StateSize
#&gt; 1      45.5   135        0      12.4          69         540   965         6
#&gt; 2      52.3   140        0      10.9          55         535  1045         6
#&gt; 3      56.6   157        1      11.2          47         512   962        22
#&gt; 4      60.3   139        1      11.9          46         480   968        19
#&gt; 5      64.2   126        0      12.2         106         599   989        40
#&gt; 6      67.6   128        0      13.5          67         624   972        28
#&gt;   HighYouthUnemploy Wage BelowWage
#&gt; 1                 1  564       139
#&gt; 2                 1  453       200
#&gt; 3                 0  288       276
#&gt; 4                 0  457       249
#&gt; 5                 1  593       171
#&gt; 6                 1  507       206
```
]
]

---

# .smaller[Exercise: Crime Rate]

.panelset[
.panel[.panel-name[Full]
We start by fitting an initial model with all covariates included (without interactions):


```r
fit.full = lm(CrimeRate ~ ., data = crime)
summary(fit.full)
```

We can see that some variables appear not significant, implying that we may be able to find a smaller model with less variables.
]
.panel[.panel-name[Back]
We can use the `step()` function to perform stepwise model selection using the AIC by removing variables iteratively from our full model. This approach is known as .hi-pink[stepwise backward AIC] and it is an heuristic method which avoids to explore ALL models.


```r
fit.aic.backward = step(fit.full, trace = F)
summary(fit.aic.backward)
```
]
.panel[.panel-name[Back]

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = CrimeRate ~ Youth + Education + Expenditure + Wage + 
#&gt;     BelowWage, data = crime)
#&gt; 
#&gt; Residuals:
#&gt;    Min     1Q Median     3Q    Max 
#&gt; -43.32 -12.69   3.12  10.78  32.52 
#&gt; 
#&gt; Coefficients:
#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept) -338.74486   90.91882  -3.726 0.000588 ***
#&gt; Youth          0.78508    0.29627   2.650 0.011387 *  
#&gt; Education      4.72597    3.05412   1.547 0.129450    
#&gt; Expenditure    0.69979    0.15487   4.519  5.2e-05 ***
#&gt; Wage           0.20208    0.08097   2.496 0.016679 *  
#&gt; BelowWage      0.55952    0.15831   3.534 0.001029 ** 
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 18.55 on 41 degrees of freedom
#&gt; Multiple R-squared:  0.6326,	Adjusted R-squared:  0.5878 
#&gt; F-statistic: 14.12 on 5 and 41 DF,  p-value: 4.872e-08
```
]
.panel[.panel-name[For]
It is also possible to consider .hi-pink[a forward approach] starting from a simple model.

```r
fit.initial = lm(CrimeRate ~ 1, data = crime)
fit.aic.forward = step(fit.initial, 
                           scope = list(lower = formula(fit.initial),
                                        upper = formula(fit.full)), 
                           direction = "forward", trace = FALSE)
summary(fit.aic.forward)
```
]
.panel[.panel-name[For]

```
#&gt; 
#&gt; Call:
#&gt; lm(formula = CrimeRate ~ Expenditure + Youth + BelowWage + Wage + 
#&gt;     Education, data = crime)
#&gt; 
#&gt; Residuals:
#&gt;    Min     1Q Median     3Q    Max 
#&gt; -43.32 -12.69   3.12  10.78  32.52 
#&gt; 
#&gt; Coefficients:
#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept) -338.74486   90.91882  -3.726 0.000588 ***
#&gt; Expenditure    0.69979    0.15487   4.519  5.2e-05 ***
#&gt; Youth          0.78508    0.29627   2.650 0.011387 *  
#&gt; BelowWage      0.55952    0.15831   3.534 0.001029 ** 
#&gt; Wage           0.20208    0.08097   2.496 0.016679 *  
#&gt; Education      4.72597    3.05412   1.547 0.129450    
#&gt; ---
#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#&gt; 
#&gt; Residual standard error: 18.55 on 41 degrees of freedom
#&gt; Multiple R-squared:  0.6326,	Adjusted R-squared:  0.5878 
#&gt; F-statistic: 14.12 on 5 and 41 DF,  p-value: 4.872e-08
```
]
.panel[.panel-name[Comp]

```r
AIC(fit.full)
```

```
#&gt; [1] 421.6054
```

```r
AIC(fit.aic.backward)
```

```
#&gt; [1] 415.4911
```

```r
AIC(fit.aic.forward)
```

```
#&gt; [1] 415.4911
```

In this case, the two methods (i.e. forward and backward) appear to be equivalent.
]
.panel[.panel-name[Check]

&lt;img src="main_files/figure-html/unnamed-chunk-37-1.svg" width="85%" style="display: block; margin: auto;" /&gt;

```
#&gt; ******************************************************************
#&gt; 	      Summary of the Quantile Residuals
#&gt;                            mean   =  8.147808e-18 
#&gt;                        variance   =  1.021739 
#&gt;                coef. of skewness  =  -0.315342 
#&gt;                coef. of kurtosis  =  2.376207 
#&gt; Filliben correlation coefficient  =  0.9879257 
#&gt; ******************************************************************
```
]
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
